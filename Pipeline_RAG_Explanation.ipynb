{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "226a43ad",
   "metadata": {},
   "source": [
    "# RAG con Hugging Face, FAISS y LangChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9468d36",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\maria\\MROSA\\BOOTCAMP IA\\Proyectos\\Proyecto_16_LLM\\Generador-de-contenido_MRosa\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import HuggingFaceDatasetLoader\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "from transformers import AutoTokenizer, pipeline\n",
    "from langchain_community.llms.huggingface_pipeline import HuggingFacePipeline\n",
    "from langchain.chains import RetrievalQA\n",
    "import pandas as pd\n",
    "\n",
    "from langchain.schema import Document\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Preparación de Datos\n",
    "El código trabaja con un conjunto de datos alojado en Hugging Face (`databricks-dolly-15k`), que contiene ejemplos de instrucciones y respuestas. Se transforman las filas del conjunto de datos en objetos `Document` para facilitar su manejo en las herramientas de LangChain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_json(\"hf://datasets/databricks/databricks-dolly-15k/databricks-dolly-15k.jsonl\", lines=True)\n",
    "data = [\n",
    "    Document(\n",
    "        metadata={\n",
    "            \"instruction\": row[\"instruction\"],\n",
    "            \"response\": row[\"response\"],\n",
    "            \"category\": row[\"category\"]\n",
    "        },\n",
    "        page_content=row[\"context\"]\n",
    "    )\n",
    "    for _, row in df.iterrows()\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "07e7f9d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>instruction</th>\n",
       "      <th>context</th>\n",
       "      <th>response</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>When did Virgin Australia start operating?</td>\n",
       "      <td>Virgin Australia, the trading name of Virgin A...</td>\n",
       "      <td>Virgin Australia commenced services on 31 Augu...</td>\n",
       "      <td>closed_qa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Which is a species of fish? Tope or Rope</td>\n",
       "      <td></td>\n",
       "      <td>Tope</td>\n",
       "      <td>classification</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Why can camels survive for long without water?</td>\n",
       "      <td></td>\n",
       "      <td>Camels use the fat in their humps to keep them...</td>\n",
       "      <td>open_qa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Alice's parents have three daughters: Amy, Jes...</td>\n",
       "      <td></td>\n",
       "      <td>The name of the third daughter is Alice</td>\n",
       "      <td>open_qa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>When was Tomoaki Komorida born?</td>\n",
       "      <td>Komorida was born in Kumamoto Prefecture on Ju...</td>\n",
       "      <td>Tomoaki Komorida was born on July 10,1981.</td>\n",
       "      <td>closed_qa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>If I have more pieces at the time of stalemate...</td>\n",
       "      <td>Stalemate is a situation in chess where the pl...</td>\n",
       "      <td>No. \\nStalemate is a drawn position. It doesn'...</td>\n",
       "      <td>information_extraction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Given a reference text about Lollapalooza, whe...</td>\n",
       "      <td>Lollapalooza /ˌlɒləpəˈluːzə/ (Lolla) is an ann...</td>\n",
       "      <td>Lollapalooze is an annual musical festival hel...</td>\n",
       "      <td>closed_qa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Who gave the UN the land in NY to build their HQ</td>\n",
       "      <td></td>\n",
       "      <td>John D Rockerfeller</td>\n",
       "      <td>open_qa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Why mobile is bad for human</td>\n",
       "      <td></td>\n",
       "      <td>We are always engaged one phone which is not g...</td>\n",
       "      <td>brainstorming</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Who was John Moses Browning?</td>\n",
       "      <td>John Moses Browning (January 23, 1855 – Novemb...</td>\n",
       "      <td>John Moses Browning is one of the most well-kn...</td>\n",
       "      <td>information_extraction</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         instruction  \\\n",
       "0         When did Virgin Australia start operating?   \n",
       "1           Which is a species of fish? Tope or Rope   \n",
       "2     Why can camels survive for long without water?   \n",
       "3  Alice's parents have three daughters: Amy, Jes...   \n",
       "4                    When was Tomoaki Komorida born?   \n",
       "5  If I have more pieces at the time of stalemate...   \n",
       "6  Given a reference text about Lollapalooza, whe...   \n",
       "7   Who gave the UN the land in NY to build their HQ   \n",
       "8                        Why mobile is bad for human   \n",
       "9                       Who was John Moses Browning?   \n",
       "\n",
       "                                             context  \\\n",
       "0  Virgin Australia, the trading name of Virgin A...   \n",
       "1                                                      \n",
       "2                                                      \n",
       "3                                                      \n",
       "4  Komorida was born in Kumamoto Prefecture on Ju...   \n",
       "5  Stalemate is a situation in chess where the pl...   \n",
       "6  Lollapalooza /ˌlɒləpəˈluːzə/ (Lolla) is an ann...   \n",
       "7                                                      \n",
       "8                                                      \n",
       "9  John Moses Browning (January 23, 1855 – Novemb...   \n",
       "\n",
       "                                            response                category  \n",
       "0  Virgin Australia commenced services on 31 Augu...               closed_qa  \n",
       "1                                               Tope          classification  \n",
       "2  Camels use the fat in their humps to keep them...                 open_qa  \n",
       "3            The name of the third daughter is Alice                 open_qa  \n",
       "4         Tomoaki Komorida was born on July 10,1981.               closed_qa  \n",
       "5  No. \\nStalemate is a drawn position. It doesn'...  information_extraction  \n",
       "6  Lollapalooze is an annual musical festival hel...               closed_qa  \n",
       "7                                John D Rockerfeller                 open_qa  \n",
       "8  We are always engaged one phone which is not g...           brainstorming  \n",
       "9  John Moses Browning is one of the most well-kn...  information_extraction  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. División de Textos\n",
    "Se utiliza un divisor de texto (`RecursiveCharacterTextSplitter`) para partir los documentos en fragmentos manejables, garantizando que cada fragmento no sea demasiado grande para el modelo y conservando cierto solapamiento para el contexto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=150)\n",
    "docs = text_splitter.split_documents(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Generación de Embeddings\n",
    "Los embeddings son representaciones vectoriales de texto que permiten comparar similitudes. Aquí, se usa un modelo preentrenado de Hugging Face para convertir los fragmentos de texto en embeddings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelPath = \"sentence-transformers/all-mpnet-base-v2\"\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=modelPath,\n",
    "    model_kwargs={'device': 'cuda' if torch.cuda.is_available() else 'cpu'},\n",
    "    encode_kwargs={'normalize_embeddings': False},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Indexación con FAISS\n",
    "Los embeddings generados se almacenan en un índice FAISS, una biblioteca diseñada para búsquedas rápidas de similitud en grandes conjuntos de vectores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "54b9397f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HuggingFaceEmbeddings(model_name='sentence-transformers/all-mpnet-base-v2', cache_folder=None, model_kwargs={'device': 'cpu'}, encode_kwargs={'normalize_embeddings': False}, multi_process=False, show_progress=False)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs_sample = docs[:100]  # Trabaja con los primeros 10 documentos\n",
    "db = FAISS.from_documents(docs_sample, embeddings)\n",
    "db.embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Recuperación de Documentos Relevantes\n",
    "Cuando se plantea una pregunta, el sistema usa FAISS para buscar fragmentos de texto relacionados en el índice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\maria\\AppData\\Local\\Temp\\ipykernel_14020\\2567618508.py:2: LangChainDeprecationWarning: The method `BaseRetriever.get_relevant_documents` was deprecated in langchain-core 0.1.46 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  docs = retriever.get_relevant_documents(\"What is Machine learning?\") # Probamos retriever con una frase\n"
     ]
    }
   ],
   "source": [
    "retriever = db.as_retriever(search_kwargs={\"k\": 4})\n",
    "docs = retriever.get_relevant_documents(\"What is Machine learning?\") # Probamos retriever con una frase"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58211704",
   "metadata": {},
   "source": [
    "+ Convierte el índice FAISS en un buscador (retriever).\n",
    "\n",
    "+ Busca los 4 documentos más relevantes en el índice usando la pregunta \"What is Machine learning?\".\n",
    "\n",
    "    **¿Por qué se hace esto?** El objetivo es acotar la cantidad de información que el modelo debe procesar. En lugar de darle todos los documentos, solo le pasas los 4 más relevantes. \n",
    "\n",
    "    Esto mejora:\n",
    "\n",
    "    + Eficiencia: El modelo de lenguaje trabaja con menos texto.\n",
    "\n",
    "    + Precisión: Se enfoca en el contexto más relevante para generar respuestas.\n",
    "\n",
    "+ Devuelve una lista de documentos que contienen información relacionada, que luego puede usarse como contexto para responder la pregunta.\n",
    "\n",
    "Este paso es crucial en sistemas de Recuperación Augmentada Generativa (RAG), donde la calidad del contexto recuperado afecta directamente la precisión de las respuestas generadas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Generación de Respuestas\n",
    "Una vez recuperado el contexto, se usa un modelo de lenguaje (en este caso, `Intel/dynamic_tinybert`) para responder preguntas directamente. El pipeline de Hugging Face se configura para responder preguntas con base en el contexto recuperado:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wine is an alcoholic drink typically made from fermented grapes. Yeast consumes the sugar in the grapes and converts it to ethanol and carbon dioxide, releasing heat in the process. Different varieties of grapes and strains of yeasts are major factors in different styles of wine. These differences result from the complex interactions between the biochemical development of the grape, the reactions involved in fermentation, the grape's growing environment (terroir), and the wine production process. Many countries enact legal appellations intended to define styles and qualities of wine. These typically restrict the geographical origin and permitted varieties of grapes, as well as other aspects of wine production. Wines can be made by fermentation of other fruit crops such as plum, cherry, pomegranate, blueberry, currant and elderberry.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\maria\\AppData\\Local\\Temp\\ipykernel_14020\\1633195007.py:24: LangChainDeprecationWarning: The class `HuggingFacePipeline` was deprecated in LangChain 0.0.37 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFacePipeline``.\n",
      "  llm = HuggingFacePipeline(\n"
     ]
    }
   ],
   "source": [
    "question = \"What is cheesemaking?\"\n",
    "searchDocs = db.similarity_search(question)\n",
    "print(searchDocs[0].page_content)\n",
    "\n",
    "# Specify the model name you want to use\n",
    "# model_name = \"Intel/dynamic_tinybert\"\n",
    "\n",
    "model_name = \"Intel/dynamic_tinybert\"\n",
    "\n",
    "\n",
    "# Load the tokenizer associated with the specified model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, padding=True, truncation=True, max_length=512)\n",
    "\n",
    "# Define a question-answering pipeline using the model and tokenizer\n",
    "question_answerer = pipeline(\n",
    "    \"question-answering\",\n",
    "    model=model_name,\n",
    "    tokenizer=tokenizer,\n",
    "    return_tensors=\"pt\"\n",
    ")\n",
    "\n",
    "# Create an instance of the HuggingFacePipeline, which wraps the question-answering pipeline\n",
    "# with additional model-specific arguments (temperature and max_length)\n",
    "llm = HuggingFacePipeline(\n",
    "    pipeline=question_answerer,\n",
    "    model_kwargs={\"temperature\": 0.7, \"max_length\": 512},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "61968afc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Invalid model-index. Not loading eval results into CardData.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "recuperar datos o documentos de la base de datos In machine learning, support vector machines (SVMs, also support vector networks) are supervised learning models with associated learning algorithms that analyze data for classification and regression analysis. Developed at AT&T Bell Laboratories by Vladimir Vapnik with colleagues (Boser et al., 1992, Guyon et al., 1993, Cortes and Vapnik, 1995, Vapnik et al., 1997[citation needed]) SVMs are one of the most robust prediction methods, being based on statistical learning frameworks or VC theory proposed by Vapnik (1982, 1995) and Chervonenkis (1974). Given a set of training examples, each marked as belonging to one of two categories, an SVM training algorithm builds a model that assigns new examples to one category or the other, making it a non-probabilistic binary linear classifier (although methods such as Platt scaling exist to use SVM in a probabilistic classification setting). SVM maps training examples to points in space so as to maximise the width of the gap between the two\n",
      "retriever tags=['FAISS', 'HuggingFaceEmbeddings'] vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x000001C794CA1F70> search_kwargs={'k': 4}\n",
      "qa verbose=False combine_documents_chain=RefineDocumentsChain(verbose=False, initial_llm_chain=LLMChain(verbose=False, prompt=PromptTemplate(input_variables=['context_str', 'question'], input_types={}, partial_variables={}, template='Context information is below. \\n------------\\n{context_str}\\n------------\\nGiven the context information and not prior knowledge, answer the question: {question}\\n'), llm=HuggingFacePipeline(pipeline=<transformers.pipelines.question_answering.QuestionAnsweringPipeline object at 0x000001C79132A240>, model_kwargs={'temperature': 0.7, 'max_length': 512}), output_parser=StrOutputParser(), llm_kwargs={}), refine_llm_chain=LLMChain(verbose=False, prompt=PromptTemplate(input_variables=['context_str', 'existing_answer', 'question'], input_types={}, partial_variables={}, template=\"The original question is as follows: {question}\\nWe have provided an existing answer: {existing_answer}\\nWe have the opportunity to refine the existing answer (only if needed) with some more context below.\\n------------\\n{context_str}\\n------------\\nGiven the new context, refine the original answer to better answer the question. If the context isn't useful, return the original answer.\"), llm=HuggingFacePipeline(pipeline=<transformers.pipelines.question_answering.QuestionAnsweringPipeline object at 0x000001C79132A240>, model_kwargs={'temperature': 0.7, 'max_length': 512}), output_parser=StrOutputParser(), llm_kwargs={}), document_variable_name='context_str', initial_response_name='existing_answer', document_prompt=PromptTemplate(input_variables=['page_content'], input_types={}, partial_variables={}, template='{page_content}')) retriever=VectorStoreRetriever(tags=['FAISS', 'HuggingFaceEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x000001C794CA1F70>, search_kwargs={'k': 4})\n"
     ]
    }
   ],
   "source": [
    "# Crea un objeto retriever desde 'db' usando el método 'as_retriever'.\n",
    "# Este retriever probablemente se usa para recuperar datos o documentos de la base de datos.\n",
    "retriever = db.as_retriever(search_kwargs={\"k\": 4})\n",
    "\n",
    "docs = retriever.get_relevant_documents(\"What is Machine learning?\")\n",
    "print(f'recuperar datos o documentos de la base de datos {docs[0].page_content}')\n",
    "\n",
    "# mejora el rendimiento en algún conjunto de tareas. Se ve como una parte de la inteligencia artificial.\n",
    "# Los algoritmos de aprendizaje automático construyen un modelo basado en ...\n",
    "\n",
    "# Crea un objeto retriever desde 'db' con una configuración de búsqueda donde recupera hasta 4 divisiones/documentos relevantes.\n",
    "retriever = db.as_retriever(search_kwargs={\"k\": 4})\n",
    "print(f'retriever {retriever}')\n",
    "\n",
    "# Crea una instancia de preguntas y respuestas (qa) usando la clase RetrievalQA.\n",
    "# Está configurado con un modelo de lenguaje (llm), un tipo de cadena \"refine\", el retriever que creamos y una opción para no devolver documentos de origen.\n",
    "qa = RetrievalQA.from_chain_type(llm=llm, chain_type=\"refine\", retriever=retriever, return_source_documents=False)\n",
    "print(f'qa {qa}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b8adb702",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context: Thomas Jefferson (April 13, 1743 – July 4, 1826) was an American statesman, diplomat, lawyer, architect, philosopher, and Founding Father who served as the third president of the United States from 1801 to 1809. Among the Committee of Five charged by the Second Continental Congress with authoring the Declaration of Independence, Jefferson was the Declaration's primary author. Following the American Revolutionary War and prior to becoming the nation's third president in 1801, Jefferson was the first United States secretary of state under George Washington and then the nation's second vice president under John Adams.\n"
     ]
    }
   ],
   "source": [
    "# Recuperar los documentos relevantes\n",
    "question = \"Who is Thomas Jefferson?\"\n",
    "docs = retriever.get_relevant_documents(question)\n",
    "\n",
    "# Extraer el contenido del primer documento como contexto\n",
    "if docs:\n",
    "    context_str = docs[0].page_content\n",
    "    print(\"Context:\", context_str)\n",
    "else:\n",
    "    print(\"No relevant documents found.\")\n",
    "    context_str = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1dce8a74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: an American statesman, diplomat, lawyer, architect, philosopher, and Founding Father\n"
     ]
    }
   ],
   "source": [
    "if context_str:  # Nos aseguramos de qu hay un contexto disponible\n",
    "    result = question_answerer(question=question, context=context_str)\n",
    "    print(\"Answer:\", result[\"answer\"])\n",
    "else:\n",
    "    print(\"No context available for answering the question.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Integración y Función Final\n",
    "La función `answer_question` encapsula todo el proceso:\n",
    "\n",
    "1. Recupera documentos relevantes.\n",
    "2. Usa el contenido del documento más relevante como contexto.\n",
    "3. Genera la respuesta basada en este contexto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_question(question, retriever, question_answerer):\n",
    "    \"\"\"\n",
    "    Responde una pregunta utilizando un retriever para obtener el contexto y un question_answerer para generar la respuesta.\n",
    "\n",
    "    Args:\n",
    "        question (str): La pregunta que se desea responder.\n",
    "        retriever: El objeto retriever para obtener documentos relevantes.\n",
    "        question_answerer: El pipeline de Hugging Face para generar respuestas.\n",
    "\n",
    "    Returns:\n",
    "        str: La respuesta generada o un mensaje indicando que no se encontró contexto.\n",
    "    \"\"\"\n",
    "    # Recuperar documentos relevantes\n",
    "    docs = retriever.get_relevant_documents(question)\n",
    "\n",
    "    # Extraer el contenido del primer documento como contexto\n",
    "    if docs:\n",
    "        context_str = docs[0].page_content\n",
    "        # print(\"Context:\", context_str)\n",
    "    else:\n",
    "        print(\"No relevant documents found.\")\n",
    "        context_str = \"\"\n",
    "\n",
    "    # Generar respuesta si hay contexto\n",
    "    if context_str:\n",
    "        result = question_answerer(question=question, context=context_str)\n",
    "        return result[\"answer\"]\n",
    "    else:\n",
    "        return \"No context available for answering the question.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Ejecución Final\n",
    "Finalmente, el sistema responde preguntas, como esta:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: an American statesman, diplomat, lawyer, architect, philosopher, and Founding Father\n"
     ]
    }
   ],
   "source": [
    "question = \"Who is Thomas Jefferson?\"\n",
    "answer = answer_question(question, retriever, question_answerer)\n",
    "print(\"Answer:\", answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5f6ef75e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: Kumamoto Prefecture\n"
     ]
    }
   ],
   "source": [
    "question = \"Where Tomoaki Komorida born?\"\n",
    "answer = answer_question(question, retriever, question_answerer)\n",
    "print(\"Answer:\", answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2d5235e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: 31 August 2000\n"
     ]
    }
   ],
   "source": [
    "question = \"When did Virgin Australia starts?\"\n",
    "answer = answer_question(question, retriever, question_answerer)\n",
    "print(\"Answer:\", answer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
